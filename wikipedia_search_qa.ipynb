{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Search Wikipedia with Vector Search and LM Question Answering\n",
        "\n",
        "This is a question answering/search strategy that breaks large, complex, wikipedia articles into little sections that can be searched through and used to answer questions. A variety of models are used in order to make sure that the answer provided is as accurate as possible. The initial code was borrowed from [this](https://huggingface.co/spaces/LectureExchange/open_domain_qa) huggingface space."
      ],
      "metadata": {
        "id": "e47H9BoQ_VCY"
      },
      "id": "e47H9BoQ_VCY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6605a669",
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2022-12-04T17:13:00.320591Z",
          "iopub.status.busy": "2022-12-04T17:13:00.319405Z",
          "iopub.status.idle": "2022-12-04T17:14:05.038197Z",
          "shell.execute_reply": "2022-12-04T17:14:05.036926Z",
          "shell.execute_reply.started": "2022-12-04T17:13:00.320497Z"
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": false,
          "start_time": "2022-12-04T18:26:04.512696",
          "status": "running"
        },
        "tags": [],
        "cellView": "form",
        "id": "6605a669"
      },
      "outputs": [],
      "source": [
        "#@title Required Installations\n",
        "#@markdown The kernel must be reset after the sentencepiece library is installed \n",
        "!pip install torch -q\n",
        "!pip install torch scipy -q \n",
        "!pip install torch pandas -q\n",
        "!pip install torch numpy -q\n",
        "!pip install torch transformers -q\n",
        "!pip install wikipedia -q\n",
        "!pip install sentence_transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Session must be restarted after installing sentencepiece\n",
        "!pip install sentencepiece -q"
      ],
      "metadata": {
        "id": "C0lRqX5F7Rzd"
      },
      "id": "C0lRqX5F7Rzd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1925b36",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-04T17:14:05.049568Z",
          "iopub.status.busy": "2022-12-04T17:14:05.047745Z",
          "iopub.status.idle": "2022-12-04T17:14:12.366285Z",
          "shell.execute_reply": "2022-12-04T17:14:12.365129Z",
          "shell.execute_reply.started": "2022-12-04T17:14:05.049522Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "cellView": "form",
        "id": "f1925b36"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import time\n",
        "import hashlib\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering, pipeline\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "from scipy.special import softmax\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import wikipedia\n",
        "from  transformers  import  AutoTokenizer, AutoModelWithLMHead, pipeline, AutoModelForTokenClassification\n",
        "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "571ad2d6",
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2022-12-04T17:14:12.369558Z",
          "iopub.status.busy": "2022-12-04T17:14:12.368426Z",
          "iopub.status.idle": "2022-12-04T17:16:29.073654Z",
          "shell.execute_reply": "2022-12-04T17:16:29.072311Z",
          "shell.execute_reply.started": "2022-12-04T17:14:12.369514Z"
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "571ad2d6",
        "outputId": "b5f71e2f-10c1-4b67-87f5-8b6eb7f1e0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/auto/modeling_auto.py:1177: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#@title Loading Models\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\").to(device).eval()\n",
        "tokenizer_ans = AutoTokenizer.from_pretrained(\"deepset/roberta-large-squad2\")\n",
        "model_ans = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-large-squad2\").to(device).eval()\n",
        "\n",
        "model_name = \"MaRiOrOsSi/t5-base-finetuned-question-answering\"\n",
        "mctokenizer = RobertaTokenizer.from_pretrained(\"LIAMF-USP/aristo-roberta\")\n",
        "mcmodel = RobertaForMultipleChoice.from_pretrained(\"LIAMF-USP/aristo-roberta\")\n",
        "t5tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "t5model = AutoModelWithLMHead.from_pretrained(model_name).to(device)\n",
        "\n",
        "embmodel = SentenceTransformer('msmarco-MiniLM-L-6-v3').to(device)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "if device == 'cuda:0':\n",
        "    pipe = pipeline(\"question-answering\",model_ans,tokenizer =tokenizer_ans,device = 0)\n",
        "else:\n",
        "    pipe = pipeline(\"question-answering\",model_ans,tokenizer =tokenizer_ans)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cls_pooling(model_output):\n",
        "    return model_output.last_hidden_state[:,0]\n",
        "\n",
        "def encode_query(query):\n",
        "    encoded_input = tokenizer(query, truncation=True, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input, return_dict=True)\n",
        "    embeddings = cls_pooling(model_output)\n",
        "    return embeddings.cpu()"
      ],
      "metadata": {
        "id": "MEfjEDINF_Ww"
      },
      "id": "MEfjEDINF_Ww",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First pass: breaking articles into searchable chunks\n",
        "def create_corpus(titles):\n",
        "  texts = []\n",
        "  for title in titles:\n",
        "    text = wikipedia.page(title).content\n",
        "    texts.append(text)\n",
        "  corpus = []\n",
        "  for text in texts:\n",
        "    section = \"\"\n",
        "    size = 0\n",
        "    for sent in nlp(text).sents:\n",
        "      section += sent.text\n",
        "      size += 1\n",
        "      if size == 30:\n",
        "        corpus.append(section)\n",
        "        size = 0\n",
        "        section = \"\"\n",
        "  embeddings = embmodel.encode(corpus,convert_to_tensor=True)\n",
        "  embeddings = embeddings.to(device)\n",
        "  return corpus, embeddings"
      ],
      "metadata": {
        "id": "BMfvswqJdQSy"
      },
      "id": "BMfvswqJdQSy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second pass: Breaking sections of articles into smaller chunks for QA\n",
        "def encode_docs(docs,maxlen = 64, stride = 32):\n",
        "    encoded_input = []\n",
        "    embeddings = []\n",
        "    spans = []\n",
        "    file_names = []\n",
        "    name, text = docs\n",
        "    \n",
        "    temp_text = \"\"\n",
        "    \n",
        "    text = text.split(\" \")\n",
        "    if len(text) < maxlen:\n",
        "        text = \" \".join(text)\n",
        "        \n",
        "        encoded_input.append(tokenizer(temp_text, return_tensors='pt', truncation = True).to(device))\n",
        "        spans.append(temp_text)\n",
        "        file_names.append(name)\n",
        "\n",
        "    else:\n",
        "        num_iters = int(len(text)/maxlen)+1\n",
        "        for i in range(num_iters):\n",
        "            if i == 0:\n",
        "                temp_text = \" \".join(text[i*maxlen:(i+1)*maxlen+stride])\n",
        "            else:\n",
        "                temp_text = \" \".join(text[(i-1)*maxlen:(i)*maxlen][-stride:] + text[i*maxlen:(i+1)*maxlen])\n",
        "\n",
        "            encoded_input.append(tokenizer(temp_text, return_tensors='pt', truncation = True).to(device))\n",
        "            spans.append(temp_text)\n",
        "            file_names.append(name)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoded in tqdm(encoded_input): \n",
        "            model_output = model(**encoded, return_dict=True)\n",
        "            embeddings.append(cls_pooling(model_output))\n",
        "    \n",
        "    embeddings = np.float32(torch.stack(embeddings).transpose(0, 1).cpu())\n",
        "    \n",
        "    np.save(\"emb_{}.npy\".format(name),dict(zip(list(range(len(embeddings))),embeddings))) \n",
        "    np.save(\"spans_{}.npy\".format(name),dict(zip(list(range(len(spans))),spans)))\n",
        "    np.save(\"file_{}.npy\".format(name),dict(zip(list(range(len(file_names))),file_names)))\n",
        "    \n",
        "    return embeddings, spans, file_names"
      ],
      "metadata": {
        "id": "UK3smvHrGMrP"
      },
      "id": "UK3smvHrGMrP",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding chunks of articles with highest QA score and search relevance\n",
        "def create_table(query,data):\n",
        "\n",
        "    k=5\n",
        "\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    \n",
        "    text = data\n",
        "    text = text.replace(\"\\r\", \" \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\" . \",\" \")\n",
        "\n",
        "    doc_emb, doc_text, file_names = encode_docs((\"name_to_save\",text),maxlen = 64, stride = 32)\n",
        "\n",
        "    doc_emb = doc_emb.reshape(-1, 768)\n",
        "    with open(\"{}.txt\".format(\"name_to_save\"),\"w\",encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    \n",
        "    #once embeddings are calculated, run MIPS\n",
        "    start = time.time()\n",
        "    query_emb = encode_query(query)\n",
        "    \n",
        "    scores = np.matmul(query_emb, doc_emb.transpose(1,0))[0].tolist()\n",
        "    doc_score_pairs = list(zip(doc_text, scores, file_names))\n",
        "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    probs_sum = 0\n",
        "    probs = softmax(sorted(scores,reverse = True)[:k])\n",
        "    table = {\"Passage\":[],\"Answer\":[],\"Probabilities\":[], \"Probs\":[]}\n",
        "    \n",
        "    \n",
        "    #get answers for each pair of question (from user) and top best passages\n",
        "    for i, (passage, _, names) in enumerate(doc_score_pairs[:k]):\n",
        "        passage = passage.replace(\"\\n\",\"\")\n",
        "        \n",
        "        if probs[i] > 0.1 or (i < 3 and probs[i] > 0.05): #generate answers for more likely passages but no less than 2\n",
        "            QA = {'question':query,'context':passage}\n",
        "            ans = pipe(QA)\n",
        "            probabilities = \"P(a|p): {}, P(a|p,q): {}, P(p|q): {}\".format(round(ans[\"score\"],5), \n",
        "                                                                          round(ans[\"score\"]*probs[i],5), \n",
        "                                                                          round(probs[i],5))\n",
        "            prob = round(probs[i],5)\n",
        "            table[\"Passage\"].append(passage)\n",
        "            table[\"Answer\"].append(str(ans[\"answer\"]))\n",
        "            table[\"Probabilities\"].append(probabilities)\n",
        "            table[\"Probs\"].append(prob)\n",
        "        else:\n",
        "            table[\"Passage\"].append(passage)\n",
        "            table[\"Answer\"].append(\"no_answer_calculated\")\n",
        "            table[\"Probabilities\"].append(\"P(p|q): {}\".format(round(probs[i],5)))\n",
        "            table[\"Probs\"].append(prob)\n",
        "            \n",
        "    return table"
      ],
      "metadata": {
        "id": "iNC9ONS1GoaN"
      },
      "id": "iNC9ONS1GoaN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e48ef5c4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-04T17:16:32.865083Z",
          "iopub.status.busy": "2022-12-04T17:16:32.864712Z",
          "iopub.status.idle": "2022-12-04T17:16:33.131275Z",
          "shell.execute_reply": "2022-12-04T17:16:33.130338Z",
          "shell.execute_reply.started": "2022-12-04T17:16:32.865045Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "e48ef5c4"
      },
      "outputs": [],
      "source": [
        "# Testing the top n results with a multiple choice model\n",
        "def predict(query,data):\n",
        "    table = create_table(query,data)\n",
        "    df = pd.DataFrame(table)\n",
        "    choices = []\n",
        "    prompts = []\n",
        "    confs = []\n",
        "    contexts = []\n",
        "    c = \"\"\n",
        "    top_n = 3\n",
        "    for i in range(top_n):\n",
        "        answer = df[\"Answer\"].tolist()[i]\n",
        "        context = df[\"Passage\"].tolist()[i]\n",
        "        conf = df[\"Probs\"].tolist()[i]\n",
        "        confs.append(conf)\n",
        "        doc = nlp(context)\n",
        "        sents = [str(sent) for sent in doc.sents]\n",
        "        for sent in sents:\n",
        "            if answer in sent and sent.lower() not in c.lower():\n",
        "                c += \" \"+sent\n",
        "        \n",
        "        if answer!= 'no_answer_calculated':\n",
        "            prompts.append(query)\n",
        "            choices.append(answer)\n",
        "            contexts.append(context)\n",
        "        p = df[\"Probabilities\"].tolist()[i]\n",
        "        question = query\n",
        "        \n",
        "    try:\n",
        "        while(c[0]==\" \"):\n",
        "                c = c[1:]\n",
        "    except:\n",
        "        pass\n",
        "    c = c.replace(\"  \", \" \").replace(\"  \", \" \")\n",
        "    prompts = [c+\" \"+prompt[:512-len(c)] for prompt in prompts]\n",
        "    labels = torch.tensor(0).unsqueeze(0)\n",
        "    encoding = mctokenizer(prompts, choices, return_tensors=\"pt\", padding=True)\n",
        "    outputs = mcmodel(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n",
        "\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits\n",
        "    \n",
        "    index = torch.argmax(logits[0])\n",
        "    \n",
        "    return choices[index], c, confs[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c24d27",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-04T17:39:42.161604Z",
          "iopub.status.busy": "2022-12-04T17:39:42.161191Z",
          "iopub.status.idle": "2022-12-04T17:39:42.176393Z",
          "shell.execute_reply": "2022-12-04T17:39:42.175049Z",
          "shell.execute_reply.started": "2022-12-04T17:39:42.161564Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "09c24d27"
      },
      "outputs": [],
      "source": [
        "# Performs vector search on large chunks from first pass to get top results, then does QA testing on smaller chunks of the top results\n",
        "def find_answer(query, corpus_embeddings, corpus):\n",
        "    top_k = 5 if len(corpus)> 5 else len(corpus)\n",
        "    query_embedding = embmodel.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "    c = \"\"\n",
        "    prompts = []\n",
        "    choices = []\n",
        "    confs = []\n",
        "    sections =[]\n",
        "    article = \"\"\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        article += \" \" + corpus[idx]\n",
        "    while len(article) < 1000:\n",
        "        article = article*2\n",
        "    answer, context, conf = predict(query, article)\n",
        "    return answer, context, conf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus, corpus_embeddings = create_corpus([\"Mexico\", \"France\"])"
      ],
      "metadata": {
        "id": "utBc54g_8YdE"
      },
      "id": "utBc54g_8YdE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0714c6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-04T17:39:42.183536Z",
          "iopub.status.busy": "2022-12-04T17:39:42.181054Z",
          "iopub.status.idle": "2022-12-04T17:39:46.680684Z",
          "shell.execute_reply": "2022-12-04T17:39:46.679533Z",
          "shell.execute_reply.started": "2022-12-04T17:39:42.183495Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fb0714c6",
        "outputId": "013a32c5-ff1e-4d3e-ba58-d14a1e51a5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 56/56 [00:01<00:00, 44.04it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'natural spring water'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "query = \"What is France's biggest export?\"\n",
        "\n",
        "answer, context, conf = find_answer(query, corpus_embeddings,corpus)\n",
        "\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0-wD0qK-Fvir",
        "outputId": "d0c6b7d2-c518-4fc2-d5eb-e360440253cd"
      },
      "id": "0-wD0qK-Fvir",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It is the world's top exporter of natural spring water, flax, malt, and potatoes. Less than 2 percent of GDP is generated by the primary sector, namely agriculture; however, France's agricultural sector is among the largest in value and leads the EU in terms of overall production. Despite protectionist policies over certain industries, particularly in agriculture, France has generally\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "papermill": {
      "default_parameters": {},
      "duration": null,
      "end_time": null,
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-12-04T18:25:56.841904",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}